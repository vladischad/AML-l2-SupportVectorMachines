{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS334 Lab2: Support Vector Machines\n",
    "\n",
    "**Total:** 100 points  \n",
    "\n",
    "This lab covers the core ideas from the SVM chapter: large margin intuition, feature scaling, soft-margin (C), kernels (RBF γ), and when to choose `LinearSVC` vs `SVC` vs `SGDClassifier`.\n",
    "\n",
    "## Learning goals\n",
    "- Train a linear SVM with proper scaling and interpret the `decision_function()` score.\n",
    "- See how **C** and **γ (gamma)** control the flexibility of an RBF-kernel SVM.\n",
    "- Compare `LinearSVC`, `SVC`, and `SGDClassifier` in practice and connect results to time complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup.\n",
    "Run the cell below once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris, make_moons, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Helper function to plot decision regions\n",
    "def plot_decision_regions(clf, X, y, title=None, ax=None, h=0.02):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0\n",
    "    y_min, y_max = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = clf.predict(grid).reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.25)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 — Linear SVM + scaling + confidence scores (30 pts)\n",
    "**Goal:** Train a linear SVM to detect *Iris virginica* using petal length/width.\n",
    "\n",
    "Steps:\n",
    "1. Load iris. Use only the 2 features: **petal length** and **petal width**.\n",
    "2. Split into train/test (80/20), with random_state=42 and stratify=y.\n",
    "3. [10 pts] Build a pipeline: `StandardScaler()` → `LinearSVC(C=1)`. Train the pipeline and print train/test accuracy.\n",
    "4. [10 pts] Compute `decision_function()` for the two points: `[[5.5, 1.7], [5.0, 1.5]]`, and print the results.\n",
    "5. [10 pts] Explain what the sign and magnitude of `decision_function()` mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data and use two features [\"petal length (cm)\", \"petal width (cm)\"]\n",
    "# and only use the last two classes (1 and 2)\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = (iris.target == 2)  # 1 = virginica, 0 = not virginica\n",
    "\n",
    "# Split into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Your code: Build a pipeline: StandardScaler() -> LinearSVC(C=1)\n",
    "\n",
    "\n",
    "# Your code: Train the pipeline, report train/test accuracy.\n",
    "\n",
    "\n",
    "# Your code: Compute decision_function() for the two points: [[5.5, 1.7], [5.0, 1.5]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer to 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 — RBF kernel: tuning **γ (gamma)** and **C** (30 pts)\n",
    "**Goal:** See underfitting vs overfitting by sweeping γ and C.\n",
    "\n",
    "Steps:\n",
    "1. (code provided) Use moons data, and split train/test.\n",
    "2. [10 pts] Train `SVC(kernel='rbf')` on a small grid:\n",
    "   - `gamma` in `{0.1, 1, 10}`\n",
    "   - `C` in `{0.1, 1, 100}`\n",
    "3. [10 pts] For each combo, report train accuracy and test accuracy. Use pandas to display results in a table (sorted by gamma and C).\n",
    "4. [10 pts] Pick one underfitting and one overfitting setting and explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "gammas = [0.1, 1, 10]\n",
    "Cs = [0.1, 1, 100]\n",
    "\n",
    "# Your code: train SVC with rbf kernel on a small grid\n",
    "\n",
    "# Your code: for each combo, report train accuracy and test accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer to 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 — Choosing `LinearSVC` vs `SVC` vs `SGDClassifier` (40 pts)\n",
    "**Goal:** Compare accuracy and training time, then connect to computational complexity.\n",
    "\n",
    "Steps:\n",
    "1. Create a linearly separable dataset with *many* points: e.g.\n",
    "   `make_classification(n_samples=8000, n_features=20, n_informative=10, class_sep=1.5, random_state=42)`.\n",
    "2. [10 pts] Train these three models (each with scaling):\n",
    "   - `LinearSVC(C=1)`\n",
    "   - `SVC(kernel='linear', C=1)`\n",
    "   - `SVC(kernel='rbf', C=1)`\n",
    "   - `SGDClassifier(loss='hinge', alpha=1e-4)`\n",
    "3. [10 pts] Measure training time and test accuracy. Save in a dataframe (3 columns: model, train_time_sec, test_acc), display the dataframe.\n",
    "4. [20 pts] Briefly justify which model you’d choose for:\n",
    "   - (a) huge dataset (millions of rows)\n",
    "   - (b) small/medium dataset but potentially nonlinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=8000,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=2,\n",
    "    class_sep=1.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "def fit_time_and_acc(model, X_train, y_train, X_test, y_test):\n",
    "    t0 = time.perf_counter()\n",
    "    model.fit(X_train, y_train)\n",
    "    t1 = time.perf_counter()\n",
    "    acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    return (t1 - t0), acc\n",
    "\n",
    "# Your code: train models (each with scaling)\n",
    "\n",
    "# Your code: Measure training time and test accuracy for each model, save in a dataframe (3 columns: model, train_time_sec, test_acc), display the dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer to 4:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs233",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
